{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0f2f81",
   "metadata": {},
   "source": [
    "## getting video ID from youtube URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cffef64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _extract_youtube_id_from_text(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Try to find a YouTube video id in the text.\n",
    "    Supports full URLs and short links.\n",
    "    \"\"\"\n",
    "    # Direct ID pattern fallback\n",
    "    youtube_id_pattern = r\"(?:v=|be/|embed/|shorts/)([A-Za-z0-9_-]{11})\"\n",
    "    match = re.search(youtube_id_pattern, text)\n",
    "    if match:\n",
    "        k = match.group(1)\n",
    "        print(k)\n",
    "        return k\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e4d885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i6dNbr582XQ\n"
     ]
    }
   ],
   "source": [
    "id = _extract_youtube_id_from_text(\"https://www.youtube.com/watch?v=i6dNbr582XQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab5d81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df51f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vide0_id=id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb811b71",
   "metadata": {},
   "source": [
    "## youtube transcipt API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d39e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "\n",
    "def fetch_transcript(video_id):\n",
    "    try :\n",
    "        ytt_api.fetch(video_id)\n",
    "        print(\"âœ…Transcript fetched successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"â€¼ï¸ðŸ¤¬An error occurred: {e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff68cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…Transcript fetched successfully.\n"
     ]
    }
   ],
   "source": [
    "video_id = \"i6dNbr582XQ\"\n",
    "snippets = fetch_transcript(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b47cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytt_api = YouTubeTranscriptApi()\n",
    "fetched_transcript = ytt_api.fetch('i6dNbr582XQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aa883c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angular is shipping updates with a\n",
      "lightning speed. But who has time to\n",
      "read every single pull request to keep\n",
      "yourself up to date? Of course, you can\n",
      "rely on blog posts, videos, and courses\n",
      "about new changes. But this is all\n",
      "delayed and this is not always accurate.\n",
      "And if you want to stay uptoate or even\n",
      "stay ahead of the coming changes, you\n",
      "have to read the source code manually.\n",
      "So I built an AI agent with Google ADK\n",
      "that scans Angular GitHub repository and\n",
      "provides me a clean summary of the pull\n",
      "requests that are about to land and I'm\n",
      "publishing this agent to GitHub. So you\n",
      "can use it. You can clone the\n",
      "repository, pull it and use it locally\n",
      "to keep yourself up to date or even\n",
      "integrate into your workflows to notify\n",
      "you on messengers for example Slack or\n",
      "Telegram or paste this information to\n",
      "notion. And in this video I want to\n",
      "break down how it's built and here is\n",
      "how it works. So this is the whole ADK\n",
      "agent which has a name. It has a model.\n",
      "We are using Gemini 2.0 flesh which is a\n",
      "simple but quick model perfect for\n",
      "frequent polling. And it has a\n",
      "description that is agent that scans\n",
      "Angular's GitHub p requests for recent\n",
      "features. And the agent has an\n",
      "instruction what to do with the\n",
      "information it gets and a tool. And a\n",
      "tool is a callable Python function that\n",
      "searches the GitHub repository itself.\n",
      "As you can see, it gets the pull request\n",
      "URL as an input. And this URL is a\n",
      "GitHub URL for pull requests that are\n",
      "that contain feature in the name and are\n",
      "open. And of course, you can configure\n",
      "it for any GitHub repository, any\n",
      "technology you use and not only open but\n",
      "also merge pull requests. So it fetches\n",
      "the HTML page, parses it with a\n",
      "beautiful soup Python library and reads\n",
      "all the links so that it can summarize\n",
      "them later. It also gets the title of\n",
      "the pull request and the description and\n",
      "provides the entry in a JSON object. And\n",
      "in case no pull requests with feature\n",
      "could be found, it throws an error. And\n",
      "we provide this function as a tool to\n",
      "the agent. So that agent knows that for\n",
      "this task, it can call this tool because\n",
      "the tool has the description. It it\n",
      "fetches the PR listing page and returns\n",
      "the scripted feature data. This is how\n",
      "the agent is working and I will leave\n",
      "the link to it in the description, a\n",
      "link to repository and you can clone it\n",
      "and use it locally. And if you like the\n",
      "content about AI, subscribe to my\n",
      "channel and more is coming\n"
     ]
    }
   ],
   "source": [
    "parts = [snippet.text for snippet in fetched_transcript]\n",
    "# is iterable\n",
    "for snippet in fetched_transcript:\n",
    "    print (snippet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2176e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Angular is shipping updates with a lightning speed. But who has time to read every single pull request to keep yourself up to date? Of course, you can rely on blog posts, videos, and courses about new changes. But this is all delayed and this is not always accurate. And if you want to stay uptoate or even stay ahead of the coming changes, you have to read the source code manually. So I built an AI agent with Google ADK that scans Angular GitHub repository and provides me a clean summary of the pull requests that are about to land and I'm publishing this agent to GitHub. So you can use it. You can clone the repository, pull it and use it locally to keep yourself up to date or even integrate into your workflows to notify you on messengers for example Slack or Telegram or paste this information to notion. And in this video I want to break down how it's built and here is how it works. So this is the whole ADK agent which has a name. It has a model. We are using Gemini 2.0 flesh which is a simple but quick model perfect for frequent polling. And it has a description that is agent that scans Angular's GitHub p requests for recent features. And the agent has an instruction what to do with the information it gets and a tool. And a tool is a callable Python function that searches the GitHub repository itself. As you can see, it gets the pull request URL as an input. And this URL is a GitHub URL for pull requests that are that contain feature in the name and are open. And of course, you can configure it for any GitHub repository, any technology you use and not only open but also merge pull requests. So it fetches the HTML page, parses it with a beautiful soup Python library and reads all the links so that it can summarize them later. It also gets the title of the pull request and the description and provides the entry in a JSON object. And in case no pull requests with feature could be found, it throws an error. And we provide this function as a tool to the agent. So that agent knows that for this task, it can call this tool because the tool has the description. It it fetches the PR listing page and returns the scripted feature data. This is how the agent is working and I will leave the link to it in the description, a link to repository and you can clone it and use it locally. And if you like the content about AI, subscribe to my channel and more is coming\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = \" \".join(parts)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c67381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_youtube_transcript(video_id: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Fetch transcript for a given video_id using youtube-transcript-api.\n",
    "    Returns raw transcript text or None on failure.\n",
    "    \"\"\"\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "    try:\n",
    "        fetched_transcript = ytt_api.fetch(video_id)\n",
    "        snippets = [snippet.text for snippet in fetched_transcript]\n",
    "        return \" \".join(snippets).strip()\n",
    "\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80f645a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i6dNbr582XQ\n",
      "Angular is shipping updates with a lightning speed. But who has time to read every single pull request to keep yourself up to date? Of course, you can rely on blog posts, videos, and courses about new changes. But this is all delayed and this is not always accurate. And if you want to stay uptoate or even stay ahead of the coming changes, you have to read the source code manually. So I built an AI agent with Google ADK that scans Angular GitHub repository and provides me a clean summary of the pull requests that are about to land and I'm publishing this agent to GitHub. So you can use it. You can clone the repository, pull it and use it locally to keep yourself up to date or even integrate into your workflows to notify you on messengers for example Slack or Telegram or paste this information to notion. And in this video I want to break down how it's built and here is how it works. So this is the whole ADK agent which has a name. It has a model. We are using Gemini 2.0 flesh which is a simple but quick model perfect for frequent polling. And it has a description that is agent that scans Angular's GitHub p requests for recent features. And the agent has an instruction what to do with the information it gets and a tool. And a tool is a callable Python function that searches the GitHub repository itself. As you can see, it gets the pull request URL as an input. And this URL is a GitHub URL for pull requests that are that contain feature in the name and are open. And of course, you can configure it for any GitHub repository, any technology you use and not only open but also merge pull requests. So it fetches the HTML page, parses it with a beautiful soup Python library and reads all the links so that it can summarize them later. It also gets the title of the pull request and the description and provides the entry in a JSON object. And in case no pull requests with feature could be found, it throws an error. And we provide this function as a tool to the agent. So that agent knows that for this task, it can call this tool because the tool has the description. It it fetches the PR listing page and returns the scripted feature data. This is how the agent is working and I will leave the link to it in the description, a link to repository and you can clone it and use it locally. And if you like the content about AI, subscribe to my channel and more is coming\n"
     ]
    }
   ],
   "source": [
    "id = _extract_youtube_id_from_text(\"analuze dj slc https://www.youtube.com/watch?v=i6dNbr582XQ\")\n",
    "transcript = _fetch_youtube_transcript(id)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6269410b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rabaada bad rnac            k kma sl lml'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"rabaada bad rnac            k kma sl lml                  \"\n",
    "cleaned = test.strip()\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe91543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insightflow (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
